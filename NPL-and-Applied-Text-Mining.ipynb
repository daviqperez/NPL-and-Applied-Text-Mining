{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2551b2-97fe-42a0-bd39-b58b1a60e84f",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment we worked with messy medical data and used regex to extract relevant information from the data. \n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats. The goal of this assignment is to correctly identify all of the different date variants encoded in this dataset and to properly normalize and sort the dates according to the following rules:\n",
    "* Assume all dates in xx/xx/xx format are mm/dd/yy\n",
    "* Assume all dates where year is encoded in only two digits are years from the 1900's (e.g. 1/5/89 is January 5th, 1989)\n",
    "* If the day is missing (e.g. 9/2009), assume it is the first day of the month (e.g. September 1, 2009).\n",
    "* If the month is missing (e.g. 2010), assume it is the first of January of that year (e.g. January 1, 2010).\n",
    "* Watch out for potential typos as this is a raw, real-life derived dataset.\n",
    "\n",
    "With these rules in mind, find the correct date in each note and return a pandas Series in chronological order of the original Series' indices. This Series should be sorted by a tie-break sort in the format of (\"extracted date\", \"original row number\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b234f11b-f93d-4970-881e-7062d73023c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         03/25/93 Total time of visit (in minutes):\\n\n",
       "1                       6/18/85 Primary Care Doctor:\\n\n",
       "2    sshe plans to move as of 7/8/71 In-Home Servic...\n",
       "3                7 on 9/27/75 Audit C Score Current:\\n\n",
       "4    2/6/96 sleep studyPain Treatment Pain Level (N...\n",
       "5                    .Per 7/06/79 Movement D/O note:\\n\n",
       "6    4, 5/18/78 Patient's thoughts about current su...\n",
       "7    10/24/89 CPT Code: 90801 - Psychiatric Diagnos...\n",
       "8                         3/7/86 SOS-10 Total Score:\\n\n",
       "9             (4/10/71)Score-1Audit C Score Current:\\n\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850d051b-6ea5-494c-9277-75c797924bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1971-04-10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971-05-18</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971-07-08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1971-07-11</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-09-12</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2016-05-30</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2016-10-13</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  index\n",
       "0   1971-04-10      9\n",
       "1   1971-05-18     84\n",
       "2   1971-07-08      2\n",
       "3   1971-07-11     53\n",
       "4   1971-09-12     28\n",
       "..         ...    ...\n",
       "495 2016-05-01    427\n",
       "496 2016-05-30    141\n",
       "497 2016-10-13    186\n",
       "498 2016-10-19    161\n",
       "499 2016-11-01    413\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_sorter():\n",
    "    # Define the regular expression pattern to extract dates\n",
    "    import re\n",
    "    pattern = r'''\n",
    "            (\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}         # Match date format 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "            |                                      # OR\n",
    "            (?:\\d{1,2}\\s)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\\s.,-]*(?:\\d{1,2}[a-z]*[\\s,-]*)?\\d{4} # Match date format Mar-20-2009;20 Mar 2009; Mar 20th; Feb 2009 etc                          \n",
    "            |                                      # OR\n",
    "            \\d{1,2}[/]\\d{2,4}                      # Match date format MM/YYYY or MM/YY\n",
    "            |                                      # OR\n",
    "            (?<!-)\\d{4})                           # Match date format YYYY\n",
    "        '''\n",
    "    # Extract dates from the text using the regular expression\n",
    "    date = df.str.extract(pattern, re.VERBOSE)\n",
    "    # Replace two-digit years with corresponding 1900s years\n",
    "    date = date.replace(r'/(\\d{2})$', r'/19\\1', regex=True)\n",
    "    # fixing mispellings\n",
    "    date = date.replace(r'Janaury', r'January', regex=True)\n",
    "    date = date.replace(r'Decemeber', r'December', regex=True)\n",
    "\n",
    "    # Convert the dates to datetime format for sorting\n",
    "    dates = pd.to_datetime(date[0], errors='coerce')\n",
    "\n",
    "    # Create a DataFrame with dates and the original index\n",
    "    df_dates = pd.DataFrame({'date': dates, 'index': df.index})\n",
    "    # Sort the DataFrame first by date and then by index in ascending order\n",
    "    sorted_df = df_dates.sort_values(by=['date', 'index'], ascending=[True, True]).reset_index(drop=True)\n",
    "    \n",
    "    return sorted_df \n",
    "\n",
    "date_sorter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82973581-86ef-4cc0-ae52-a02ec85fd71f",
   "metadata": {},
   "source": [
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In this assignment we created a spelling recommender function that uses nltk to find words similar to the misspelling. For every misspelled word, the recommender should find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a31abe-a086-4156-8949-ed01931f98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4981d58a-268e-4b65-abee-5d7c8b516d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    # your code goes here\n",
    "    from nltk.metrics.distance import jaccard_distance\n",
    "    from nltk.util import ngrams\n",
    "    # YOUR CODE HERE\n",
    "    outcomes = []\n",
    "    for entry in entries:\n",
    "        spellings = [s for s in correct_spellings if s.startswith(entry[0])]\n",
    "        distances = ((jaccard_distance(set(ngrams(entry, 3)),\n",
    "                                       set(ngrams(word, 3))), word) for word in spellings)\n",
    "        closest = min(distances)\n",
    "        outcomes.append(closest[1])\n",
    "    return outcomes# Your answer here\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832af9c9-157e-4491-b4ff-f566cbde8b20",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "In this assignment we explored text message data and created models to predict if a message is spam or not. The task was the following:\n",
    "\n",
    "Fit and transform the **first 2000 rows** of training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.** To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100 and max_iter=1000. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple. The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first. The three features that were added to the document term matrix should have the following names should they appear in the list of coefficients:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba20fef-4cb5-462f-892e-47bae04cf01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "179588cf-4042-4549-8e29-47cfe154a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92feca6-2d52-4b2a-ae37-65df15a77d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9975680355839261,\n",
       " ['n ', ' i', 'at', 'he', ' m', '..', 'us', 'go', ' lo', ' bu'],\n",
       " ['digit_count', 'ne', ' st', 'co', 's ', 'xt', 'lt', 'xt ', ' ne', 'der'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def answer_eleven():\n",
    "    \n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train[:2000])\n",
    "    X_train_vectorized = vect.transform(X_train[:2000]) \n",
    "    \n",
    "    # Calculate the length of documents, number of digits, and non-word characters, and add them as features\n",
    "    length_of_doc = [len(doc) for doc in X_train[:2000]]\n",
    "    digit_count = [len(re.findall(r'\\d', doc)) for doc in X_train[:2000]]\n",
    "    non_word_char_count = [len(re.findall(r'\\W', doc)) for doc in X_train[:2000]]\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, [length_of_doc,digit_count,non_word_char_count])\n",
    "\n",
    "    # Fit the Logistic Regression model\n",
    "    model = LogisticRegression(C=100, max_iter=1000)\n",
    "    model.fit(X_train_vectorized, y_train[:2000])\n",
    "\n",
    "    # Transform the test data and calculate AUC score\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    length_of_doc = [len(doc) for doc in X_test]\n",
    "    digit_count = [len(re.findall(r'\\d', doc)) for doc in X_test]\n",
    "    non_word_char_count = [len(re.findall(r'\\W', doc)) for doc in X_test]    \n",
    "    X_test_vectorized = add_feature(X_test_vectorized, [length_of_doc,digit_count,non_word_char_count])\n",
    "\n",
    "    probabilities = model.predict_proba(X_test_vectorized)[:, 1]\n",
    "    \n",
    "    # Combine CountVectorizer feature names with custom feature names\n",
    "    count_vectorizer_feature_names = np.array(vect.get_feature_names_out())\n",
    "    custom_feature_names = ['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "    feature_names = np.concatenate((count_vectorizer_feature_names, custom_feature_names))\n",
    "    \n",
    "    # Find the 10 smallest and 10 largest coefficients from the model\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "    smallest_coefs = list(feature_names[sorted_coef_index[:10]])\n",
    "    largest_coefs = list(feature_names[sorted_coef_index[:-11:-1]])\n",
    "    \n",
    "    return roc_auc_score(y_test, probabilities), smallest_coefs, largest_coefs\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411cd2b-0749-4863-9556-d838a3864580",
   "metadata": {},
   "source": [
    "# Assignment 4 - Document Similarity & Topic Modelling\n",
    "\n",
    "For  this assignment, we used Gensim's LDA (Latent Dirichlet Allocation) model to model topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f060a0-1ba8-4268-ab61-59ccc22bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the list of documents\n",
    "with open('newsgroups', 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)\n",
    "\n",
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(newsgroup_data)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4d1fe8-6323-4996-8562-7d93f0009a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate LDA model parameters on the corpus\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word = id_map, passes = 25, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df4c7e1-738e-4d99-8f0a-4172c882fc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.056*\"edu\" + 0.043*\"com\" + 0.033*\"thanks\" + 0.022*\"mail\" + 0.021*\"know\" + 0.020*\"does\" + 0.014*\"info\" + 0.012*\"monitor\" + 0.010*\"looking\" + 0.010*\"don\"'),\n",
       " (1,\n",
       "  '0.024*\"ground\" + 0.018*\"current\" + 0.018*\"just\" + 0.013*\"want\" + 0.013*\"use\" + 0.011*\"using\" + 0.011*\"used\" + 0.010*\"power\" + 0.010*\"speed\" + 0.010*\"output\"'),\n",
       " (2,\n",
       "  '0.061*\"drive\" + 0.042*\"disk\" + 0.033*\"scsi\" + 0.030*\"drives\" + 0.028*\"hard\" + 0.028*\"controller\" + 0.027*\"card\" + 0.020*\"rom\" + 0.018*\"floppy\" + 0.017*\"bus\"'),\n",
       " (3,\n",
       "  '0.023*\"time\" + 0.015*\"atheism\" + 0.014*\"list\" + 0.013*\"left\" + 0.012*\"alt\" + 0.012*\"faq\" + 0.012*\"probably\" + 0.011*\"know\" + 0.011*\"send\" + 0.010*\"months\"'),\n",
       " (4,\n",
       "  '0.025*\"car\" + 0.016*\"just\" + 0.014*\"don\" + 0.014*\"bike\" + 0.012*\"good\" + 0.011*\"new\" + 0.011*\"think\" + 0.010*\"year\" + 0.010*\"cars\" + 0.010*\"time\"'),\n",
       " (5,\n",
       "  '0.030*\"game\" + 0.027*\"team\" + 0.023*\"year\" + 0.017*\"games\" + 0.016*\"play\" + 0.012*\"season\" + 0.012*\"players\" + 0.012*\"win\" + 0.011*\"hockey\" + 0.011*\"good\"'),\n",
       " (6,\n",
       "  '0.017*\"information\" + 0.014*\"help\" + 0.014*\"medical\" + 0.012*\"new\" + 0.012*\"use\" + 0.012*\"000\" + 0.012*\"research\" + 0.011*\"university\" + 0.010*\"number\" + 0.010*\"program\"'),\n",
       " (7,\n",
       "  '0.022*\"don\" + 0.021*\"people\" + 0.018*\"think\" + 0.017*\"just\" + 0.012*\"say\" + 0.011*\"know\" + 0.011*\"does\" + 0.011*\"good\" + 0.010*\"god\" + 0.009*\"way\"'),\n",
       " (8,\n",
       "  '0.034*\"use\" + 0.023*\"apple\" + 0.020*\"power\" + 0.016*\"time\" + 0.015*\"data\" + 0.015*\"software\" + 0.012*\"pin\" + 0.012*\"memory\" + 0.012*\"simms\" + 0.011*\"port\"'),\n",
       " (9,\n",
       "  '0.068*\"space\" + 0.036*\"nasa\" + 0.021*\"science\" + 0.020*\"edu\" + 0.019*\"data\" + 0.017*\"shuttle\" + 0.015*\"launch\" + 0.015*\"available\" + 0.014*\"center\" + 0.014*\"sci\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lda_topics():\n",
    "    \"\"\"\n",
    "    find a list of the 10 topics and the most significant 10 words in each topic\n",
    "    \"\"\"\n",
    "    return ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "\n",
    "lda_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1d0b2-b080-4ace-ba28-401579f462b3",
   "metadata": {},
   "source": [
    "Also, we Found the topic distribution for the new document `new_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d762bd-9ccb-41fb-8ab1-bfd3c0885f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3348d473-d8ce-4181-85f1-d25114a0af11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.020003106),\n",
       " (1, 0.020003323),\n",
       " (2, 0.02000128),\n",
       " (3, 0.4967471),\n",
       " (4, 0.020004036),\n",
       " (5, 0.020004127),\n",
       " (6, 0.02000297),\n",
       " (7, 0.020002643),\n",
       " (8, 0.020003127),\n",
       " (9, 0.34322825)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_distribution():\n",
    "    \n",
    "    new_doc_transformed = vect.transform(new_doc)\n",
    "    corpus = gensim.matutils.Sparse2Corpus(new_doc_transformed, documents_columns=False)\n",
    "    doc_topics = ldamodel.get_document_topics(corpus)\n",
    "    topic_dist = []\n",
    "    for val in list(doc_topics):\n",
    "        for v in val:\n",
    "            topic_dist.append(v)\n",
    "    return topic_dist\n",
    "\n",
    "topic_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852875a0-7a3d-42f6-a646-18c9e12665ce",
   "metadata": {},
   "source": [
    "it is obvious that the most relevant topics are topic 3 (which is not clear to what is related) and topic 9 which is related Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
